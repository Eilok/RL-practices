### 框架

- 鹅：蒙特卡洛树搜索
- 狐狸：PPO
狐狸通过与鹅的对抗，不断优化自己的策略，从而提高狐狸的胜率。

### 环境设计
空间和动作都是离散的

- 状态空间: 7*7的棋盘
- 动作空间：(假设狐狸每轮只能移动一步)
    1. 上
    2. 上右
    3. 右
    4. 下右
    5. 下
    6. 下左
    7. 左
    8. 上左
    9. 上两格
    10. 上右两格
    11. 右两格
    12. 下右两格
    13. 下两格
    14. 下左两格
    15. 左两格
    16. 上左两格

- 奖励：
    1. 狐狸获胜：+100
    2. 狐狸失败: -100
    3. 狐狸吃子：+20
    4. 走位所得：
        ```Python
        [
            [0, 0, -2, -3, -2, 0, 0],
            [0, 0, -2, -1, -2, 0, 0],
            [-2, -2, -1, -1, -1, -2, -2],
            [-3, -1, -1, -1, -1, -1, -3],
            [-2, -2, -1, -1, -1, -2, -2],
            [0, 0, -2, -1, -2, 0, 0],
            [0, 0, -2, -3, -2, 0, 0]
        ]
        ```

### 训练方法

在狐狸的每一轮训练中，狐狸需要和鹅进行对抗，因此需要从外部修改环境的棋局。

鹅用蒙特卡洛树走子，鹅走完之后，狐狸用PPO进行决策，然后根据盘面狐狸会得到一定的奖励，以此不断更新自己的策略。

Q：只要让狐狸学到吃鹅就行？不需要教它多步移动？